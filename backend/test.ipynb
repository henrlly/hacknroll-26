{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba156cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type='narration' event_type='narration_generation_start' scene_number=1\n",
      "\n",
      "language_code='eng' language_probability=0.627894401550293 text='Saving this audio to a file.' words=[SpeechToTextWordResponseModel(text='Saving', start=0.199, end=0.659, type='word', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=0.659, end=0.699, type='spacing', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='this', start=0.699, end=0.879, type='word', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=0.879, end=0.939, type='spacing', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='audio', start=0.939, end=1.319, type='word', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.319, end=1.339, type='spacing', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='to', start=1.339, end=1.46, type='word', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.46, end=1.619, type='spacing', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='a', start=1.619, end=1.619, type='word', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text=' ', start=1.619, end=1.639, type='spacing', speaker_id=None, logprob=0.0, characters=None), SpeechToTextWordResponseModel(text='file.', start=1.639, end=2.24, type='word', speaker_id=None, logprob=0.0, characters=None)] channel_index=None additional_formats=None transcription_id='fTN0jqImXtLMrIZMri7O' entities=None\n",
      "\n",
      "{\"words\":[\"Saving\",\"this\",\"audio\",\"to\",\"a\"],\"word_start_times_seconds\":[0.199,0.699,0.939,1.339,1.619],\"word_end_times_seconds\":[0.659,0.879,1.319,1.46,1.619]}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import shutil\n",
    "from typing import Awaitable, Callable\n",
    "\n",
    "from elevenlabs import AsyncElevenLabs\n",
    "from fishaudio import AsyncFishAudio\n",
    "from fishaudio.utils import play, save\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from app.api.types import AssetResponse, NarrationResponse\n",
    "from app.core.config import app_config, settings\n",
    "from app.core.models import VoiceType\n",
    "from app.services.narration_timestamps import (\n",
    "    extract_word_timestamps,\n",
    "    transcribe_with_scribe_v2,\n",
    ")\n",
    "from app.services.trump_voice import generate_trump_voice\n",
    "from app.utils.alignment import (\n",
    "    character_alignment_to_word_alignment,\n",
    "    mock_word_alignment,\n",
    "    stt_response_to_character_alignment,\n",
    ")\n",
    "from app.utils.normalize_volume import normalize_volume\n",
    "\n",
    "client = AsyncElevenLabs(\n",
    "    api_key=settings.ELEVENLABS_API_KEY, base_url=\"https://api.elevenlabs.io\"\n",
    ")\n",
    "\n",
    "fishClient = AsyncFishAudio(api_key=settings.FISH_API_KEY)\n",
    "\n",
    "async def generate_speech(\n",
    "    text: str,\n",
    "    voice: VoiceType,\n",
    "    file_name: str,\n",
    "    callback: Callable[[BaseModel], Awaitable],\n",
    "    scene_number: int,\n",
    "    mock: bool = True,\n",
    "    use_flash: bool = app_config.MOCK_NARRATION,\n",
    "    use_local: bool = app_config.USE_LOCAL_TTS,\n",
    ") -> str:\n",
    "    await callback(\n",
    "        NarrationResponse(\n",
    "            event_type=\"narration_generation_start\", scene_number=scene_number\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if mock:\n",
    "        shutil.copyfile(\n",
    "            \"mock/narration.mp3\",\n",
    "            file_name,\n",
    "        )\n",
    "        alignment = mock_word_alignment(text)\n",
    "    else:\n",
    "        if use_local:\n",
    "            generate_trump_voice(text, file_name)\n",
    "        else:\n",
    "            voices: dict[VoiceType, str] = {\n",
    "                \"trump\": \"7ee05bf86c884881945ca034aeddbebb\",\n",
    "                \"obama\": \"4ce7e917cedd4bc2bb2e6ff3a46acaa1\",\n",
    "                \"peter\": \"a5c5987257a14018a90111ee52a4e71a\"\n",
    "            }\n",
    "            audio = await fishClient.tts.convert(text=\"Saving this audio to a file!\", reference_id=voices[voice])\n",
    "            save(audio, file_name)\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            transcription = await client.speech_to_text.convert(\n",
    "                model_id=\"scribe_v1\", file=f\n",
    "            )\n",
    "            print()\n",
    "            print(transcription)\n",
    "            print()\n",
    "        return stt_response_to_character_alignment(transcription)\n",
    "\n",
    "    await callback(\n",
    "        NarrationResponse(\n",
    "            event_type=\"narration_generation_end\", scene_number=scene_number\n",
    "        )\n",
    "    )\n",
    "    return alignment\n",
    "\n",
    "\n",
    "async def generate_sound_effect(\n",
    "    description: str,\n",
    "    file_name: str,\n",
    "    callback: Callable[[BaseModel], Awaitable],\n",
    "    asset_id: str,\n",
    "    mock: bool = app_config.MOCK_SFX,\n",
    "):\n",
    "    await callback(\n",
    "        AssetResponse(\n",
    "            event_type=\"generation_start\",\n",
    "            asset_id=asset_id,\n",
    "            asset_type=\"sound_effect\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if mock:\n",
    "        shutil.copyfile(\n",
    "            \"mock/sfx.mp3\",\n",
    "            file_name,\n",
    "        )\n",
    "    else:\n",
    "        res = client.text_to_sound_effects.convert(text=description)\n",
    "        b = b\"\"\n",
    "        async for chunk in res:\n",
    "            b += chunk\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(b)\n",
    "        print(f\"Generated sound effect saved to {file_name}\\n\")\n",
    "\n",
    "        await normalize_volume(file_name)\n",
    "        print(f\"Normalized volume for sound effect {file_name}\\n\")\n",
    "\n",
    "    await callback(\n",
    "        AssetResponse(\n",
    "            type=\"asset\",\n",
    "            event_type=\"generation_end\",\n",
    "            asset_id=asset_id,\n",
    "            asset_type=\"sound_effect\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "async def wrapper(x):\n",
    "\n",
    "    print(x)\n",
    "\n",
    "print(await generate_speech(\"test\", mock = False, voice=\"trump\", file_name=\"test.mp3\", callback=wrapper, scene_number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from app.core.config import app_config\n",
    "from app.core.pipeline import pipeline\n",
    "from app.utils.make_callback import make_callback\n",
    "\n",
    "\n",
    "async def test():\n",
    "    async def print_wrapper(message: str):\n",
    "        print(message)\n",
    "\n",
    "    print(app_config.model_dump_json(indent=2))\n",
    "\n",
    "    topic = \"Binary Trees\"\n",
    "    callback = make_callback(print_wrapper)\n",
    "    await pipeline(callback=callback, topic=topic, voice=\"obama\")\n",
    "\n",
    "# full test run\n",
    "# await test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a11af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.core.prompts\n",
    "# edit prompt here\n",
    "\n",
    "PLAN_PROMPT_INSTRUCTIONS = \"\"\"\n",
    "You are an expert video content creator specializing in creating engaging Fireship-style explainer videos.\n",
    "\"\"\"\n",
    "\n",
    "PLAN_PROMPT_TEMPLATE = \"\"\"\n",
    "Create a detailed plan for a 20-second Fireship-style explainer video about the following topic:\n",
    "{{ topic }}\n",
    "\n",
    "# GENERAL RULES\n",
    "- There **must** be a total of 4 scenes, each 5 seconds long, for a total video duration of 20 seconds.\n",
    "- Each scene **must** represent one cut.\n",
    "- Narration should be punchy and opinionated, similar to Fireship videos.\n",
    "- The video **must** be funny and reference popular internet culture or memes where appropriate.\n",
    "- Select a main and secondary font to use for the entire video, for cohesiveness of the video. (This will be used during editing.) The fonts should be commonly available.\n",
    "- DO NOT add any branding or mention Fireship in any way.\n",
    "- DO NOT add any generic call to action (e.g., \"like and subscribe\", \"follow for more\").\n",
    "- When executing the plan, we will have access to the manim animation tool.\n",
    "\n",
    "# For each scene, provide the following details\n",
    "1. Scene number (0-indexed)\n",
    "2. Rough duration of the scene (in seconds)\n",
    "3. Description of scene visuals\n",
    "4. Narration/script for the scene\n",
    "5. Description of any sound effects or background music\n",
    "6. Edit notes (e.g., transitions, effects)\n",
    "7. List of assets needed (Visual or Sound effect)\n",
    "8. Scene structure (high-level sequence of events in the scene)\n",
    "\n",
    "# RULES FOR SCENE STRUCTURE\n",
    "- Scene structure should be a numbered list of the sequence of events in the scene.\n",
    "- Only specify exact timings for when to start the scene voiceover. Each scene has a separate voiceover audio file.\n",
    "- DO NOT specify exact timings for anything else in the scene structure.\n",
    "Example scene structure:\n",
    "```\n",
    "1. Dark background initialization\n",
    "2. Start scene voiceover immediately at t=0\n",
    "3. Animate title text \"LINKED LISTS\" at the top\n",
    "4. Sequentially introduce 4 linked list nodes from left to right:\n",
    "    - Each node appears with a pop sound\n",
    "    - Each node connects to the previous node via an arrow\n",
    "5. Emphasize \"TREASURE HUNT\" with larger text and glow\n",
    "6. End with a highlight traversal from first node to last\n",
    "```\n",
    "\n",
    "# RULES FOR VISUAL ASSETS\n",
    "- When executing the plan, we will use the manim tool to generate visuals besides just using assets.\n",
    "- Manim can generate animated graphs, text, shapes, and vector graphics.\n",
    "- DO NOT list assets that will be generated using manim.\n",
    "- DO NOT list manim as an asset.\n",
    "- DO list visual assets that cannot be generated using manim, such as:\n",
    "    - Meme or funny images or clips (e.g., surprised pikachu meme, distracted boyfriend meme, pepelaugh)\n",
    "    - Stock images or clips (e.g., a person typing on a laptop, city skyline timelapse)\n",
    "- Be liberal in listing visual assets that will enhance the video.\n",
    "- Visual assets should appear on screen for at least 1 second to allow viewers to absorb them.\n",
    "\n",
    "# RULES FOR ASSETS\n",
    "- Asset descriptions should specify EXACTLY what the asset is.\n",
    "    - GOOD description: \"A dog eating a hot dog alone\"\n",
    "    - BAD description: \"A dog eating something that seems tasty\". This is too vague.\n",
    "- Each asset description should only describe a SINGLE SPECIFIC asset. DO NOT give options.\n",
    "    - GOOD description: \"surprised pikachu meme\". This describes a single specific asset.\n",
    "    - BAD description: \"loss meme or struggling person image\". This has multiple options.\n",
    "- DO NOT give examples in the asset descriptions.\n",
    "    - BAD description: \"A meme expressing frustation (e.g., person facepalming, character looking defeated)\". This is an example, not a description.\n",
    "- Each asset should include BOTH a short and a long description.\n",
    "- Short description:\n",
    "    - Short descriptions will be used to look up assets from the asset database.\n",
    "    - Short description should be EXTREMELY short and generic and use less than 6 words. The asset database is limited and cannot handle very specific descriptions.\n",
    "    - DO NOT include words like \"image\", \"clip\", \"visual\", \"stock footage\", or \"sound effect\" in the short description.\n",
    "    - Examples of Short description (non-exhaustive): \"Surprised pikachu meme\", \"Explosion sound effect\"\n",
    "- Long description:\n",
    "    - Long description will be used as the prompt for the AI to generate the assets.\n",
    "    - Long description should be more DETAILED and SPECIFIC than the short description.\n",
    "    - Examples of Long description (non-exhaustive): \"A dog eating a hot dog alone\", \"Loud explosion sound effect with deep bass\"\n",
    "- We will compare the assets generated (with long description) with the assets in the asset database (with short description) and select the best asset that matches both descriptions.\n",
    "\n",
    "# RULES FOR NARRATION/SCRIPT\n",
    "- Narration/script should be written in a conversational tone, as if explaining to a friend.\n",
    "- Enhance the narration with humor, analogies, and relatable examples where appropriate to keep the audience engaged.\n",
    "- Enhance the narration with internet culture references and memes where appropriate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from app.api.types import (\n",
    "    FinalVideoResponse,\n",
    "    PlanStreamedResponse,\n",
    "    StartPipelineResponse,\n",
    ")\n",
    "from app.core.config import app_config\n",
    "from app.core.generation.manim.generate_code import generate_manim_code_parallel, _generate_manim_code\n",
    "from app.core.generation.manim.render_loop import render_manim_loop_parallel\n",
    "from app.core.generation.manim.render_code import _render_manim_code\n",
    "from app.core.generation.manim.stitch_scene import stitch_manim_scenes_together\n",
    "from app.core.generation.plan import generate_plan\n",
    "from app.core.generation.visuals import generate_visual_asset\n",
    "from app.core.models import Asset, PipelineCallback, VideoPlan, VoiceType\n",
    "from app.services.elevenlabs import generate_sound_effect, generate_speech\n",
    "from app.utils.gather import gather_with_concurrency\n",
    "from app.utils.get_file_path import (\n",
    "    get_narration_scene_file_path,\n",
    "    get_selected_asset_file_path,\n",
    ")\n",
    "\n",
    "topic = \"Binary Trees\"\n",
    "voice = \"obama\"\n",
    "\n",
    "async def callback(message: str, delay: float = 0):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(message)\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "os.makedirs(f\"static/{session_id}\", exist_ok=True)\n",
    "os.makedirs(f\"working/{session_id}\", exist_ok=True)\n",
    "\n",
    "# Step 1: Generate video plan\n",
    "await callback(StartPipelineResponse(session_id=session_id, success=True))\n",
    "\n",
    "plan = await generate_plan(\n",
    "    input_plan=None,\n",
    "    topic=topic,\n",
    "    streaming_delay=0,\n",
    "    callback=callback,\n",
    "    mock_plan=True,\n",
    "    chars_per_stream_message=300000,\n",
    "    plan_prompt_template=PLAN_PROMPT_TEMPLATE,\n",
    "    plan_prompt_instructions=PLAN_PROMPT_INSTRUCTIONS,\n",
    ")\n",
    "\n",
    "await callback(PlanStreamedResponse(event_type=\"plan_end\"))\n",
    "\n",
    "print(f\"\\nGenerated Plan:\\n{plan.model_dump_json(indent=2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate assets and narration\n",
    "\n",
    "# Step 2: Generate narration for each scene\n",
    "print(\"\\nGenerating narration...\\n\")\n",
    "\n",
    "scripts: list[str] = [scene.narration_script for scene in plan.scenes]\n",
    "\n",
    "# Generate narrations in parallel\n",
    "# Elevenlabs API has concurrency limits of 5\n",
    "# We're on creator tier, so 5 concurrent requests allowed for sound effects\n",
    "# 10 for narration cos flash/turbo model (using flash)\n",
    "# https://help.elevenlabs.io/hc/en-us/articles/14312733311761-How-many-requests-can-I-make-and-can-I-increase-it\n",
    "word_timings = await gather_with_concurrency(\n",
    "    5,\n",
    "    *[\n",
    "        generate_speech(\n",
    "            text=narration,\n",
    "            voice=voice,\n",
    "            file_name=get_narration_scene_file_path(\n",
    "                session_id=session_id, scene_number=i\n",
    "            ),\n",
    "            callback=callback,\n",
    "            scene_number=i,\n",
    "        )\n",
    "        for i, narration in enumerate(scripts)\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"\\nNarration_results:\\n{word_timings}\\n\")\n",
    "\n",
    "# Step 3: Generate assets and manim code for each scene\n",
    "print(\"\\nGenerating assets and manim code...\\n\")\n",
    "\n",
    "# split assets into sound effects and visual assets,\n",
    "# because sound effects have a concurrency limit of 5 on elevenlabs\n",
    "visual_assets: list[Asset] = [\n",
    "    asset\n",
    "    for scene in plan.scenes\n",
    "    for asset in scene.assets_needed\n",
    "    if asset.asset_type == \"visual\"\n",
    "]\n",
    "sound_effects: list[Asset] = [\n",
    "    asset\n",
    "    for scene in plan.scenes\n",
    "    for asset in scene.assets_needed\n",
    "    if asset.asset_type == \"sound_effect\"\n",
    "]\n",
    "\n",
    "# Max 5 concurrent requests for sound effects\n",
    "gather_sound_effects = gather_with_concurrency(\n",
    "    5,\n",
    "    *[\n",
    "        generate_sound_effect(\n",
    "            description=asset.asset_long_desc,\n",
    "            file_name=get_selected_asset_file_path(\n",
    "                session_id=session_id, asset_id=asset.asset_id, ext=\"mp3\"\n",
    "            ),\n",
    "            callback=callback,\n",
    "            asset_id=asset.asset_id,\n",
    "        )\n",
    "        for asset in sound_effects\n",
    "    ],\n",
    ")\n",
    "\n",
    "gather_visual_assets = asyncio.gather(\n",
    "    *[\n",
    "        generate_visual_asset(asset=asset, session_id=session_id, callback=callback)\n",
    "        for asset in visual_assets\n",
    "    ]\n",
    ")\n",
    "\n",
    "await asyncio.gather(\n",
    "    gather_sound_effects,\n",
    "    gather_visual_assets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.core.prompts\n",
    "# edit prompt here\n",
    "\n",
    "MANIM_PROMPT_INSTRUCTIONS = \"\"\"\n",
    "You are an expert in creating engaging \"Fireship-style\" explainer videos using the Manim Community v0.19.1 library.\n",
    "Your task is to generate Manim code for a specific scene of the video, following the detailed requirements and inputs provided.\n",
    "\"\"\"\n",
    "\n",
    "# Seperating subtitle and voiceover logic gave worse results - need to ask LLM for delay before starting captions and voiceover\n",
    "# Inserting video - https://github.com/3b1b/manim/issues/760#issuecomment-925659697\n",
    "#\n",
    "# PROBLEMS WITH MANIM PROMPT:\n",
    "# Sometimes only a part of a single word is highlighted in captions.\n",
    "# E.g. see mock scene output - \"ki*ds*\"\n",
    "# Tokenization issue? Just tell it to highlight full words\n",
    "# and not (never? unless there is a good reason)\n",
    "# only partial words/certain characters in a word\n",
    "#\n",
    "MANIM_PROMPT_TEMPLATE = \"\"\"\n",
    "# OBJECTIVE\n",
    "Generate **Manim Community v0.19.1** Python code for **Scene {{ scene_number }}** of a \"Fireship-style\" explainer video about **{{ topic }}**.\n",
    "\n",
    "# SCENE {{ scene_number }} STRUCTURE\n",
    "The scene **must** follow this exact high-level sequence:\n",
    "\n",
    "{{ scene_structure }}\n",
    "\n",
    "# OUTPUT SPECIFICATIONS\n",
    "- **Class Name:** Define the main class as `Scene{{ scene_number }}`.\n",
    "- **Format:** Output **only** the raw Python code. Do not use Markdown formatting, backticks, or code blocks.\n",
    "- **Libraries Allowed:** You have access to `manim`, `cv2`, and `numpy` only.\n",
    "\n",
    "# STYLE & ANIMATION GUIDELINES\n",
    "1.  **Fireship Style:** The scene **must** be fast-paced, engaging, funny, and high-energy.\n",
    "2.  **No Branding:** Do not mention Fireship or add any branding.\n",
    "3.  **No Call to Action:** Do not include generic calls to action (e.g., \"like and subscribe\", \"follow for more\").\n",
    "3.  **Visuals:** Use the main and secondary fonts provided in the plan. You may use other common fonts if they fit the style.\n",
    "4.  **Boundaries:** Ensure all visual elements, including animated captions, images, and text, remain strictly within the frame boundaries. No element should be clipped by the edges of the screen.\n",
    "5.  **Colors**: Do not use standard Manim color constants (e.g., RED, GREEN). You **must** strictly define color constants with this exact format: `NEW_COLOR = ManimColor.from_rgb((R, G, B), alpha=1.0)`. DO NOT define colors in any other way.\n",
    "6.  **Captions:**\n",
    "    *   Include animated captions for the voiceover.\n",
    "    *   Captions **must** be semantic phrases, not individual words. DO NOT create one caption per word.\n",
    "    *   Each caption **must** span the combined duration of its associated words.\n",
    "    *   Captions **must** appear and disappear within 0.2s of the voiceover timing\n",
    "    *   **Style:** Captions **must** have different styles (e.g., color, size, font weight) to emphasize key points, humor, or punchlines in the narration.\n",
    "    *   **Placement:** Ensure captions do not overlap with other essential visual elements and stay within frame boundaries.\n",
    "7.  **Synchronization:** Strictly align transitions, animations, effects, text, and sound effects with the provided `word_start_times_seconds` and `word_end_times_seconds`. External visual assets **must** appear on screen for at least 1 second.\n",
    "\n",
    "# ASSET HANDLING SPECIFICATIONS\n",
    "\n",
    "## 1. File Paths\n",
    "Assets are located in `static/{{ session_id }}/`.\n",
    "*   **Visuals:** `{asset_id}.mp4` (includes static images converted to single-frame mp4s).\n",
    "*   **Audio (SFX):** `{asset_id}.mp3`.\n",
    "*   **Voiceover:** `narration_scene_{{ scene_number }}.mp3` (located in the same folder).\n",
    "\n",
    "The assets for this scene are as follows:\n",
    "{{ scene_assets }}\n",
    "\n",
    "## 2. Audio Implementation\n",
    "Use `self.add_sound(\"path/to/file.mp3\")` for both the voiceover and sound effects.\n",
    "\n",
    "## 3. Visual Implementation (Custom CV2 Logic)\n",
    "All visual assets are `.mp4` files. Some are single-frame loops (images), others are video clips.\n",
    "*   **Scaling:** Resize frames to fit the scene using `frame_img.scale_to_fit_height()` or `.scale_to_fit_width()`. (Manim default: 14.22w x 8h).\n",
    "*   **Aspect Ratio:** Assume roughly square aspect ratios.\n",
    "*   **Minimum Duration**: Any external .mp4 asset **must** remain visible for a MINIMUM of 1 second. Do not cut them shorter than this, even if the voiceover is fast.\n",
    "*   **Logic:** If there are visual assets, you **must** adapt the following `cv2` pattern to display these assets, ensuring you loop or cut them to fit the scene duration:\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(\"path to mp4 asset\")\n",
    "visual_asset_duration = 1.0 # minimum duration for the visual asset in seconds (**must** be at least 1 second)\n",
    "frame_time = 0.04\n",
    "elapsed = 0\n",
    "\n",
    "current_frame = None\n",
    "while elapsed < visual_asset_duration:\n",
    "    flag, frame = cap.read()\n",
    "\n",
    "    if not flag:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        flag, frame = cap.read()\n",
    "\n",
    "    if flag:\n",
    "        if current_frame is not None:\n",
    "            self.remove(current_frame)\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_img = ImageMobject(frame).scale_to_fit_height(5)\n",
    "        frame_img.move_to(UP * 0.5)\n",
    "        current_frame = frame_img\n",
    "        self.add(frame_img)\n",
    "        self.wait(frame_time)\n",
    "        elapsed += frame_time\n",
    "    else:\n",
    "        self.wait(frame_time)\n",
    "        elapsed += frame_time\n",
    "\n",
    "cap.release()\n",
    "```\n",
    "\n",
    "# TECHNICAL CONSTRAINTS\n",
    "You **must** strictly adhere to these rules to prevent runtime errors:\n",
    "1.  **Group vs. VGroup:** `ImageMobject` is **NOT** a `VMobject`.\n",
    "    *   **Never** add an `ImageMobject` to a `VGroup`.\n",
    "    *   If you need to group images (or mix images with vectors/text), you **must** use `Group()` instead of `VGroup()`.\n",
    "2.  **Numpy Types in Text:** If numpy strings used, you **must** explicitly cast them to a Python string before passing to Manim objects.\n",
    "3.  **Code Blocks:** If code snippets are required, you **must** strictly instantiate the `Code` class using the following signature (filling in `code_string` and `language` as needed):\n",
    "    ```python\n",
    "    Code(\n",
    "        code_string=\"...\",\n",
    "        language=\"...\",\n",
    "        formatter_style='vim',\n",
    "        tab_width=4,\n",
    "        add_line_numbers=True,\n",
    "        line_numbers_from=1,\n",
    "        background='rectangle',\n",
    "        background_config=None,\n",
    "        paragraph_config=None\n",
    "    )\n",
    "    ```\n",
    "4.  **Minimum Wait Time:** You **must** ensure that the wait duration in `self.wait(duration)` is positive, by enforcing a minimum wait time of 0.01s for any calls with `self.wait(max(0.01, duration))`.\n",
    "\n",
    "\n",
    "# INPUT DATA\n",
    "\n",
    "## 1. Full Video Plan\n",
    "Use this plan for context on flow and edit notes. **Only generate code for Scene {{ scene_number }}.**\n",
    "\n",
    "{{ full_plan }}\n",
    "\n",
    "## 2. Voiceover Script (Scene {{ scene_number }})\n",
    "Use this text for caption content.\n",
    "\n",
    "{{ full_script }}\n",
    "\n",
    "## 3. Timing Data\n",
    "Use the lists below for precise alignment. Times are in seconds relative to the start of this scene.\n",
    "*   `word_start_times_seconds`: Start time of each word.\n",
    "*   `word_end_times_seconds`: End time of each word.\n",
    "\n",
    "{{ word_timings }}\n",
    "\n",
    "# TIMING SPECIFICATIONS\n",
    "*   **Source of Truth:** Use the provided `word_start_times_seconds` and `word_end_times_seconds` lists to align animations and captions with the voiceover.\n",
    "*   **Timings:** Times are relative to the start of this scene (scene {{ scene_number }}).\n",
    "*   **Precision:** Animation start times should be within ±0.1s of the narration timing.\n",
    "*   **Captions:** Captions may slightly overlap transitions for readability, but they should appear and disappear within ±0.2s of the voiceover timing.\n",
    "*   **Visuals:** All imported visual assets **must** appear on screen for at least 1 second to allow viewers to absorb them.\n",
    "*   **Native Manim Elements:**  Other Manim objects (shapes, text, code blocks, flashes) are NOT subject to the 1-second rule. These should be fast, transient, and match the high-energy \"Fireship\" pacing (e.g., appearing for only 0.8s is acceptable).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate code for just one scene, one version\n",
    "mock_code_generation = True\n",
    "scene_number = 0\n",
    "version_number = 0\n",
    "code = await _generate_manim_code(\n",
    "    scene_number=scene_number,\n",
    "    full_script=scripts[scene_number],\n",
    "    word_timings=word_timings[scene_number],\n",
    "    session_id=session_id,\n",
    "    plan=plan,\n",
    "    callback=callback,\n",
    "    version_number=version_number,\n",
    "    code_prompt_instructions=MANIM_PROMPT_INSTRUCTIONS,\n",
    "    code_prompt_template=MANIM_PROMPT_TEMPLATE,\n",
    "    mock=mock_code_generation,\n",
    ")\n",
    "\n",
    "# Print the generated code\n",
    "print(code.code)\n",
    "\n",
    "\n",
    "# Or for all scenes sequentially, but each scene's versions in parallel\n",
    "# for i in range(len(plan.scenes)):\n",
    "#     await generate_manim_code_parallel(\n",
    "#         scene_number=i,\n",
    "#         full_script=scripts[i],\n",
    "#         word_timings=word_timings[i],\n",
    "#         session_id=session_id,\n",
    "#         plan=plan,\n",
    "#         callback=callback,\n",
    "#         num_code_versions=app_config.NUM_CODE_VERSIONS_PER_SCENE,\n",
    "#         code_prompt_instructions=MANIM_PROMPT_INSTRUCTIONS,\n",
    "#         code_prompt_template=MANIM_PROMPT_TEMPLATE,\n",
    "#         mock=mock_code_generation,\n",
    "#     )\n",
    "\n",
    "\n",
    "# Or for all scenes and versions in parallel\n",
    "# gather_manim_code = asyncio.gather(\n",
    "#     *[\n",
    "#         generate_manim_code_parallel(\n",
    "#             scene_number=i,\n",
    "#             full_script=scripts[i],\n",
    "#             word_timings=word_timings[i],\n",
    "#             session_id=session_id,\n",
    "#             plan=plan,\n",
    "#             callback=callback,\n",
    "#             num_code_versions=app_config.NUM_CODE_VERSIONS_PER_SCENE,\n",
    "#             code_prompt_instructions=MANIM_PROMPT_INSTRUCTIONS,\n",
    "#             code_prompt_template=MANIM_PROMPT_TEMPLATE,\n",
    "#             mock=mock_code_generation,\n",
    "#         )\n",
    "#         for i in range(len(plan.scenes))\n",
    "#     ]\n",
    "# )\n",
    "# await gather_manim_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36872bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rendering just one scene, one version\n",
    "mock_render = False\n",
    "\n",
    "video_file_path, error_message = await _render_manim_code(\n",
    "    session_id=session_id,\n",
    "    scene_number=scene_number,\n",
    "    version_number=version_number,\n",
    "    mock=mock_render,\n",
    ")\n",
    "\n",
    "print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514eb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play scene rendered\n",
    "from IPython.display import display, Video\n",
    "\n",
    "display(Video(data=video_file_path, width=400, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28469df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14275565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
